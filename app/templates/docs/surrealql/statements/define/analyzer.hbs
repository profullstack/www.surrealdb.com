<Crumb>
	<Crumb::Item @link="docs">Documentation</Crumb::Item>
	<Crumb::Item @link="docs.surrealql">SurrealQL</Crumb::Item>
	<Crumb::Item @link="docs.surrealql.statements">Statements</Crumb::Item>
	<Crumb::Item @link="docs.surrealql.statements.define"><code>DEFINE</code> statement</Crumb::Item>
	<Crumb::Item @link="docs.surrealql.statements.define.analyzer"><code>ANALYZER</code></Crumb::Item>
</Crumb>

<Layout::Text text-l text-f>
	<h2><code>DEFINE ANALYZER</code> statement</h2>
	<p>
		In the context of a database, an analyzer plays a crucial role in text processing and searching.
		It is defined by its name, a set of tokenizers, and a collection of filters.
	</p>
	<h4>Requirements</h4>
	<ul>
		<li>You must be authenticated as a root, namespace, or database user before you can use the <code>DEFINE INDEX</code> statement.</li>
		<li><LinkTo @route="docs.surrealql.statements.use">You must select your namespace and database</LinkTo> before you can use the <code>DEFINE INDEX</code> statement.</li>
	</ul>
</Layout::Text>

<Layout::Gap mini />

<Layout::Text text-l text-f>
	<h3>Statement syntax</h3>
	<Code @name="docs/surrealql/statements/define/analyzer/syntax.typescript" text="SQL Syntax" />
</Layout::Text>

<Layout::Gap mini />

<Layout::Text text-l text-f>
	<h3>Tokenizers</h3>
	<p>Tokenizers are responsible for breaking down a given text into individual tokens based on a set of instructions. There are a couple of tokenizers that can be used while defining an analyzer as seen below: </p>
	<ul>
		<li><code>blank</code>: The blank tokenizer breaks down a text into tokens by creating a new token each time it encounters a space, tab, or newline character. It's a straightforward way to split text into words or chunks based on whitespace.</li>
		<codes vertical>
		<Code @name="doc-statement-define-analyzer-tokenizer-blank.surql">
			DEFINE ANALYZER example_blank TOKENIZERS blank;
		</Code>
		<Code @name="doc-statement-define-analyzer-tokenizer-blank.txt">
			Input Text: "Hello World" , Tokens: ["Hello", "World"]
		</Code>
		</codes>


		<li><code>camel</code>: The camel tokenizer is used for identifying and creating tokens when the next character in the text is uppercase. This is particularly useful for processing camelCase or PascalCase text, common in programming, to split them into meaningful words.</li>
		<codes vertical>
			<Code @name="doc-statement-define-analyzer-tokenizer-camel.surql">
				DEFINE ANALYZER example_camel TOKENIZERS camel;
			</Code>
			<Code @name="doc-statement-define-analyzer-tokenizer-blank.txt">
			Input Text: "helloWorld" , Tokens: ["hello", "World"]
		    </Code>
		</codes>
		<li><code>class</code>:  The class tokenizer segments text into tokens by detecting changes (digit, letter, punctuation, blank) in the Unicode class of characters. It creates a new token when the character class changes, distinguishing between digits, letters, punctuation, and blanks. This allows for flexible tokenization based on character types. </li>
		<codes vertical>
			<Code @name="doc-statement-define-analyzer-tokenizer-class.surql">
				DEFINE ANALYZER example_class TOKENIZERS class;
			</Code>
			<Code @name="doc-statement-define-analyzer-tokenizer-class.txt">
			Input Text: "123abc!XYZ", Tokens: ["123", "abc", "!", "XYZ"]
		    </Code>
		</codes>

		<li><code>punct</code>: The punct tokenizer generates tokens by breaking the text whenever a punctuation character is encountered. It's suitable for tokenizing sentences or breaking text into smaller units based on punctuation marks.</li>
		<codes vertical>
			<Code @name="doc-statement-define-analyzer-tokenizer-punct.surql">
				DEFINE ANALYZER example_punct TOKENIZERS punct;
			</Code>
			<Code @name="doc-statement-define-analyzer-tokenizer-punct.txt">
			Input Text: "Hello, World!", Tokens: ["Hello", ",", "World", "!"]
		    </Code>
		</codes>
	</ul>

</Layout::Text>

<Layout::Gap mini />

<Layout::Text text-l text-f>
	<h3>Filters</h3>
	<p>Filters take on the task of transforming these tokens for further processing and analysis.</p>
	<ul>
		<li><code>ascii</code>: The ascii filter is responsible for processing tokens by replacing or removing diacritical marks (accents and special characters) from the text. It helps standardize text by converting accented characters to their basic ASCII equivalents, making it more suitable for various text analysis tasks.</li>
        <codes vertical>
			<Code @name="doc-statement-define-analyzer-filter-ascii.surql">
				DEFINE ANALYZER example_ascii TOKENIZERS class FILTERS ascii;
			</Code>
			<Code @name="doc-statement-define-analyzer-filter-ascii.txt">
			Input Text: "résumé café" , Tokens: ["resume", "cafe"]
		    </Code>
		</codes>


		<li><code>edgengram(min,max)</code>: The edgengram filter is used to create tokens that represent prefixes of terms. It generates a sequence of tokens that gradually build up a term, which can be useful for autocomplete or searching based on partial words. It accepts two parameters <code>min</code> and <code>max</code> which define the minimum and maximum amount of characters in the prefix.</li>

		 <codes vertical>
			<Code @name="doc-statement-define-analyzer-filter-edgengram.surql">
				DEFINE ANALYZER example_edgengram TOKENIZERS class FILTERS edgengram(1,3);

			</Code>
			<Code @name="doc-statement-define-analyzer-filter-edgengram.txt">
			Input Text: "apple banana", Tokens: ["a", "ap", "app", "b", "ba", "ban"]
		    </Code>
		</codes>

		<li><code>lowercase</code>: The lowercase filter converts tokens to lowercase, ensuring that text is consistently in lowercase format. This is often used to make text case-insensitive for search and analysis purposes.</li>

        <codes vertical>
			<Code @name="doc-statement-define-analyzer-filter-lowercase.surql">
				DEFINE ANALYZER example_lowercase TOKENIZERS class FILTERS lowercase;

			</Code>
			<Code @name="doc-statement-define-analyzer-filter-lowercase.txt">
			Input Text: "Hello World", Tokens: ["hello", "world"]
		    </Code>
		</codes>


		<li>
			<code>snowball(language)</code>: The snowball filter applies Snowball stemming to tokens, reducing them to their root form and converts the case to lowercase.
			The following supported languages can be passed as a parameter in snowball:
			Arabic, Danish, Dutch, English, French, German, Greek, Hungarian, Italian,
			Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, Tamil, Turkish.
		</li>

		 <codes vertical>
			<Code @name="doc-statement-define-analyzer-filter-snowball.surql">
				DEFINE ANALYZER example_snowball TOKENIZERS class FILTERS snowball(english);
			</Code>
			<Code @name="doc-statement-define-analyzer-filter-snowball.txt">
			Input Text: "running cats", Tokens: ["run", "cat"]
		    </Code>
		</codes>

		<li><code>uppercase</code>The uppercase filter converts tokens to uppercase, ensuring text consistency in uppercase format. It can be useful when case-insensitivity is required for specific analysis or search operations.</li>

		<codes vertical>
			<Code @name="doc-statement-define-analyzer-filter-uppercase.surql">
				DEFINE ANALYZER example_uppercase TOKENIZERS class FILTERS uppercase;
			</Code>
			<Code @name="doc-statement-define-analyzer-filter-uppercase.txt">
			Input Text: "Hello World", Tokens: ["HELLO", "WORLD"]
		    </Code>
		</codes>

	</ul>
</Layout::Text>

<Layout::Gap mini />

<Layout::Text text-l text-f>
	<h3>Example usage</h3>
	<p>This example creates an analyzer that splits tokens on blank characters and removes diacritical marks.</p>
	<Code @name="docs/surrealql/statements/define/analyzer/analyzer-ascii.surql" />
	<p>This command statement creates an analyzer specifically designed for processing English texts.</p>
	<Code @name="docs/surrealql/statements/define/analyzer/analyzer-english.surql" />
	<p>This statement creates an analyzer specifically designed for auto-completion tasks.</p>
	<Code @name="docs/surrealql/statements/define/analyzer/analyzer-autocomplete.surql" />
	<p>This command statement creates an analyzer specifically designed for source code analysis.</p>
	<Code @name="docs/surrealql/statements/define/analyzer/analyzer-code.surql" />
</Layout::Text>

<Layout::Gap large />